{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Improved Water Quality Prediction Models\n",
    "\n",
    "This notebook implements a systematic experiment to improve water quality prediction\n",
    "over the baseline benchmark, addressing overfitting, feature redundancy, and model diversity.\n",
    "\n",
    "## Phases\n",
    "1. Fix evaluation methodology (5-fold CV)\n",
    "2. Feature selection + spatial clustering\n",
    "3. Target variable transformation (log1p)\n",
    "4. Improved missing value handling\n",
    "5. Model diversity and tuning (RF, XGBoost, LightGBM)\n",
    "\n",
    "## Benchmark Results (to beat)\n",
    "| Parameter | R² Test |\n",
    "|---|---|\n",
    "| Total Alkalinity | 0.546 |\n",
    "| Electrical Conductance | 0.585 |\n",
    "| Dissolved Reactive Phosphorus | 0.529 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Step 1: Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import cross_val_score, KFold, GroupKFold\n",
    "from sklearn.metrics import r2_score, mean_squared_error, make_scorer\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "N_FOLDS = 5\n",
    "TARGETS = ['Total Alkalinity', 'Electrical Conductance', 'Dissolved Reactive Phosphorus']\n",
    "\n",
    "print('Libraries loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Step 2: Load & Merge Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wq_df = pd.read_csv('water_quality_training_dataset.csv')\n",
    "landsat_train = pd.read_csv('landsat_features_training_enhanced.csv')\n",
    "landsat_val = pd.read_csv('landsat_features_validation_enhanced.csv')\n",
    "tc_train = pd.read_csv('terraclimate_features_training.csv')\n",
    "tc_val = pd.read_csv('terraclimate_features_validation.csv')\n",
    "submission_template = pd.read_csv('submission_template.csv')\n",
    "\n",
    "print(f'Water quality:    {wq_df.shape}')\n",
    "print(f'Landsat train:    {landsat_train.shape}')\n",
    "print(f'Landsat val:      {landsat_val.shape}')\n",
    "print(f'TerraClimate:     {tc_train.shape}')\n",
    "print(f'Submission:       {submission_template.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_frame(wq, landsat, terraclimate):\n",
    "    df = pd.concat([wq, landsat, terraclimate], axis=1)\n",
    "    df = df.loc[:, ~df.columns.duplicated()]\n",
    "    return df\n",
    "\n",
    "def add_temporal_features(df):\n",
    "    df = df.copy()\n",
    "    dates = pd.to_datetime(df['Sample Date'], dayfirst=True, errors='coerce')\n",
    "    df['Month'] = dates.dt.month.astype(float)\n",
    "    df['Year'] = dates.dt.year.astype(float)\n",
    "    return df\n",
    "\n",
    "train_df = build_frame(wq_df, landsat_train, tc_train)\n",
    "train_df = add_temporal_features(train_df)\n",
    "print(f'Merged training shape: {train_df.shape}')\n",
    "print(f'Columns: {list(train_df.columns)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Step 3: Handle Missing Values (Phase 4)\n",
    "\n",
    "1,085 rows (~11.6%) have all Landsat bands as NaN. Instead of naive median fill,\n",
    "we add a `landsat_missing` indicator and then impute with median so the model\n",
    "can learn to treat these rows differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "landsat_cols = ['nir', 'green', 'swir16', 'swir22', 'NDMI', 'MNDWI',\n",
    "                'NDVI', 'NDWI', 'NDSI_water', 'NDTI', 'Turbidity_Index',\n",
    "                'Chlorophyll_Proxy', 'BSI', 'SWIR22_NIR_ratio', 'SWIR16_NIR_ratio',\n",
    "                'Green_NIR_ratio', 'SWIR22_Green_ratio', 'SWIR16_Green_ratio',\n",
    "                'log_nir', 'log_green', 'log_swir16', 'log_swir22',\n",
    "                'nir_squared', 'swir22_squared']\n",
    "\n",
    "train_df['landsat_missing'] = train_df['nir'].isnull().astype(float)\n",
    "\n",
    "missing_before = train_df.isnull().sum()\n",
    "print('Missing values before imputation:')\n",
    "print(missing_before[missing_before > 0].to_string())\n",
    "print(f'\\nlandsat_missing == 1 count: {int(train_df[\"landsat_missing\"].sum())}')\n",
    "\n",
    "train_df = train_df.fillna(train_df.median(numeric_only=True))\n",
    "print(f'\\nMissing after imputation: {train_df.isnull().sum().sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Step 4: Spatial Clustering + Feature Engineering (Phase 2)\n",
    "\n",
    "KMeans on Latitude/Longitude to create spatial clusters that proxy for\n",
    "watershed/catchment identity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLUSTERS = 8\n",
    "kmeans = KMeans(n_clusters=N_CLUSTERS, random_state=RANDOM_STATE, n_init=10)\n",
    "train_df['Cluster'] = kmeans.fit_predict(train_df[['Latitude', 'Longitude']]).astype(float)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "scatter = ax.scatter(train_df['Longitude'], train_df['Latitude'],\n",
    "                     c=train_df['Cluster'], cmap='tab10', s=5, alpha=0.5)\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "ax.set_title(f'Spatial Clusters (k={N_CLUSTERS})')\n",
    "plt.colorbar(scatter, ax=ax, label='Cluster')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Cluster distribution:\\n{train_df[\"Cluster\"].value_counts().sort_index()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Step 5: Define Feature Sets\n",
    "\n",
    "**Baseline** (4 features): identical to benchmark.  \n",
    "**Curated** (~12 features): removes redundant pairs, adds spatial + missing indicator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASELINE_FEATURES = ['swir22', 'NDMI', 'MNDWI', 'pet']\n",
    "\n",
    "CURATED_FEATURES = [\n",
    "    'swir22', 'NDMI', 'MNDWI', 'pet',\n",
    "    'Latitude', 'Longitude', 'Cluster',\n",
    "    'NDVI', 'BSI', 'Chlorophyll_Proxy',\n",
    "    'Month', 'landsat_missing',\n",
    "]\n",
    "\n",
    "ALL_ENHANCED_FEATURES = [\n",
    "    'nir', 'green', 'swir16', 'swir22',\n",
    "    'NDMI', 'MNDWI', 'NDVI', 'NDSI_water',\n",
    "    'Chlorophyll_Proxy', 'BSI',\n",
    "    'SWIR22_NIR_ratio', 'SWIR16_NIR_ratio',\n",
    "    'Green_NIR_ratio', 'SWIR22_Green_ratio',\n",
    "    'pet', 'Month', 'Year',\n",
    "    'Latitude', 'Longitude', 'Cluster',\n",
    "    'landsat_missing',\n",
    "]\n",
    "\n",
    "CURATED_FEATURES = [c for c in CURATED_FEATURES if c in train_df.columns]\n",
    "ALL_ENHANCED_FEATURES = [c for c in ALL_ENHANCED_FEATURES if c in train_df.columns]\n",
    "\n",
    "print(f'Baseline features ({len(BASELINE_FEATURES)}): {BASELINE_FEATURES}')\n",
    "print(f'Curated features  ({len(CURATED_FEATURES)}): {CURATED_FEATURES}')\n",
    "print(f'All enhanced      ({len(ALL_ENHANCED_FEATURES)}): {ALL_ENHANCED_FEATURES}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Step 6: Cross-Validation Pipeline (Phase 1)\n",
    "\n",
    "Replace the single 70/30 split with 5-fold CV. Report mean +/- std for R² and RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_scorer(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "neg_rmse_scorer = make_scorer(rmse_scorer, greater_is_better=False)\n",
    "\n",
    "def cv_evaluate(model, X, y, cv=None, label='', target_name=''):\n",
    "    \"\"\"Run k-fold CV and return summary dict.\"\"\"\n",
    "    if cv is None:\n",
    "        cv = KFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "    r2_scores = cross_val_score(model, X, y, cv=cv, scoring='r2', n_jobs=-1)\n",
    "    rmse_scores = -cross_val_score(model, X, y, cv=cv, scoring=neg_rmse_scorer, n_jobs=-1)\n",
    "\n",
    "    result = {\n",
    "        'Model': label,\n",
    "        'Target': target_name,\n",
    "        'R2_mean': round(r2_scores.mean(), 4),\n",
    "        'R2_std': round(r2_scores.std(), 4),\n",
    "        'RMSE_mean': round(rmse_scores.mean(), 4),\n",
    "        'RMSE_std': round(rmse_scores.std(), 4),\n",
    "    }\n",
    "    print(f'  {label:30s} | {target_name:35s} | '\n",
    "          f'R²: {result[\"R2_mean\"]:.4f} ± {result[\"R2_std\"]:.4f} | '\n",
    "          f'RMSE: {result[\"RMSE_mean\"]:.2f} ± {result[\"RMSE_std\"]:.2f}')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Step 7: Baseline Experiment (Reproduce Benchmark with CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 110)\n",
    "print('BASELINE MODEL (swir22, NDMI, MNDWI, pet) — 5-fold CV')\n",
    "print('=' * 110)\n",
    "\n",
    "baseline_rf = RandomForestRegressor(\n",
    "    n_estimators=200, max_features='sqrt', min_samples_leaf=2,\n",
    "    random_state=RANDOM_STATE, n_jobs=-1\n",
    ")\n",
    "\n",
    "baseline_results = []\n",
    "for target in TARGETS:\n",
    "    X = train_df[BASELINE_FEATURES].values\n",
    "    y = train_df[target].values\n",
    "    res = cv_evaluate(baseline_rf, X, y, label='Baseline RF', target_name=target)\n",
    "    baseline_results.append(res)\n",
    "\n",
    "baseline_df = pd.DataFrame(baseline_results)\n",
    "print('\\nBaseline Summary:')\n",
    "baseline_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Step 8: Curated Features + RF (Phase 2 test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 110)\n",
    "print('CURATED FEATURES + RF — 5-fold CV')\n",
    "print('=' * 110)\n",
    "\n",
    "curated_results = []\n",
    "for target in TARGETS:\n",
    "    X = train_df[CURATED_FEATURES].values\n",
    "    y = train_df[target].values\n",
    "    res = cv_evaluate(baseline_rf, X, y, label='Curated RF', target_name=target)\n",
    "    curated_results.append(res)\n",
    "\n",
    "curated_df = pd.DataFrame(curated_results)\n",
    "print('\\nCurated Summary:')\n",
    "curated_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Step 9: Target Transformation — log1p (Phase 3)\n",
    "\n",
    "DRP is heavily right-skewed (15% outliers by IQR, mean 43.5 vs median 20).\n",
    "EC has high variance. log1p compresses extreme values, expm1 reverses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin, clone\n",
    "\n",
    "class LogTransformedRegressor(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"Wraps a regressor with log1p target transformation.\"\"\"\n",
    "    def __init__(self, base_estimator):\n",
    "        self.base_estimator = base_estimator\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.estimator_ = clone(self.base_estimator)\n",
    "        self.estimator_.fit(X, np.log1p(y))\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.expm1(self.estimator_.predict(X))\n",
    "\n",
    "print('=' * 110)\n",
    "print('CURATED FEATURES + log1p RF — 5-fold CV')\n",
    "print('=' * 110)\n",
    "\n",
    "log_rf = LogTransformedRegressor(baseline_rf)\n",
    "\n",
    "log_results = []\n",
    "for target in TARGETS:\n",
    "    X = train_df[CURATED_FEATURES].values\n",
    "    y = train_df[target].values\n",
    "    res = cv_evaluate(log_rf, X, y, label='Curated log1p RF', target_name=target)\n",
    "    log_results.append(res)\n",
    "\n",
    "log_df = pd.DataFrame(log_results)\n",
    "print('\\nlog1p Transform Summary:')\n",
    "log_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Step 10: Model Diversity — XGBoost & LightGBM (Phase 5)\n",
    "\n",
    "Gradient boosting models with built-in regularization handle feature\n",
    "redundancy more gracefully than RF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBRegressor(\n",
    "    n_estimators=500, learning_rate=0.05, max_depth=6,\n",
    "    subsample=0.8, colsample_bytree=0.7,\n",
    "    reg_alpha=0.1, reg_lambda=1.0,\n",
    "    random_state=RANDOM_STATE, n_jobs=-1, verbosity=0\n",
    ")\n",
    "\n",
    "lgbm_model = LGBMRegressor(\n",
    "    n_estimators=500, learning_rate=0.05, max_depth=6,\n",
    "    subsample=0.8, colsample_bytree=0.7,\n",
    "    reg_alpha=0.1, reg_lambda=1.0,\n",
    "    random_state=RANDOM_STATE, n_jobs=-1, verbose=-1\n",
    ")\n",
    "\n",
    "models = {\n",
    "    'RF':      baseline_rf,\n",
    "    'XGBoost': xgb_model,\n",
    "    'LightGBM': lgbm_model,\n",
    "}\n",
    "\n",
    "print('=' * 110)\n",
    "print('MODEL COMPARISON — Curated features, 5-fold CV (raw targets)')\n",
    "print('=' * 110)\n",
    "\n",
    "comparison_results = []\n",
    "for model_name, model in models.items():\n",
    "    for target in TARGETS:\n",
    "        X = train_df[CURATED_FEATURES].values\n",
    "        y = train_df[target].values\n",
    "        res = cv_evaluate(model, X, y, label=f'Curated {model_name}', target_name=target)\n",
    "        comparison_results.append(res)\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_results)\n",
    "print('\\nModel Comparison Summary:')\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## Step 11: Model Diversity with log1p Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_models = {\n",
    "    'log1p RF':      LogTransformedRegressor(baseline_rf),\n",
    "    'log1p XGBoost': LogTransformedRegressor(xgb_model),\n",
    "    'log1p LightGBM': LogTransformedRegressor(lgbm_model),\n",
    "}\n",
    "\n",
    "print('=' * 110)\n",
    "print('MODEL COMPARISON — Curated features, 5-fold CV (log1p targets)')\n",
    "print('=' * 110)\n",
    "\n",
    "log_comparison_results = []\n",
    "for model_name, model in log_models.items():\n",
    "    for target in TARGETS:\n",
    "        X = train_df[CURATED_FEATURES].values\n",
    "        y = train_df[target].values\n",
    "        res = cv_evaluate(model, X, y, label=f'Curated {model_name}', target_name=target)\n",
    "        log_comparison_results.append(res)\n",
    "\n",
    "log_comparison_df = pd.DataFrame(log_comparison_results)\n",
    "print('\\nlog1p Model Comparison Summary:')\n",
    "log_comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Step 12: Hyperparameter Tuning (Phase 5 continued)\n",
    "\n",
    "RandomizedSearchCV on the best-performing model family, tuned per-target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "xgb_param_dist = {\n",
    "    'base_estimator__n_estimators': randint(200, 800),\n",
    "    'base_estimator__max_depth': randint(3, 10),\n",
    "    'base_estimator__learning_rate': uniform(0.01, 0.19),\n",
    "    'base_estimator__subsample': uniform(0.6, 0.4),\n",
    "    'base_estimator__colsample_bytree': uniform(0.4, 0.6),\n",
    "    'base_estimator__reg_alpha': uniform(0, 2),\n",
    "    'base_estimator__reg_lambda': uniform(0.5, 2.5),\n",
    "}\n",
    "\n",
    "lgbm_param_dist = {\n",
    "    'base_estimator__n_estimators': randint(200, 800),\n",
    "    'base_estimator__max_depth': randint(3, 10),\n",
    "    'base_estimator__learning_rate': uniform(0.01, 0.19),\n",
    "    'base_estimator__subsample': uniform(0.6, 0.4),\n",
    "    'base_estimator__colsample_bytree': uniform(0.4, 0.6),\n",
    "    'base_estimator__reg_alpha': uniform(0, 2),\n",
    "    'base_estimator__reg_lambda': uniform(0.5, 2.5),\n",
    "}\n",
    "\n",
    "rf_param_dist = {\n",
    "    'base_estimator__n_estimators': randint(100, 500),\n",
    "    'base_estimator__max_depth': [10, 15, 20, 30, None],\n",
    "    'base_estimator__min_samples_leaf': randint(1, 10),\n",
    "    'base_estimator__max_features': ['sqrt', 'log2', 0.3, 0.5, 0.8],\n",
    "}\n",
    "\n",
    "cv_inner = KFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "best_models = {}\n",
    "tuning_results = []\n",
    "\n",
    "candidate_configs = [\n",
    "    ('log1p XGBoost', LogTransformedRegressor(xgb_model), xgb_param_dist),\n",
    "    ('log1p LightGBM', LogTransformedRegressor(lgbm_model), lgbm_param_dist),\n",
    "    ('log1p RF', LogTransformedRegressor(baseline_rf), rf_param_dist),\n",
    "]\n",
    "\n",
    "for target in TARGETS:\n",
    "    print(f'\\n{\"=\" * 80}')\n",
    "    print(f'Tuning for: {target}')\n",
    "    print(f'{\"=\" * 80}')\n",
    "\n",
    "    X = train_df[CURATED_FEATURES].values\n",
    "    y = train_df[target].values\n",
    "\n",
    "    best_score = -np.inf\n",
    "    best_name = ''\n",
    "    best_est = None\n",
    "\n",
    "    for name, base_model, param_dist in candidate_configs:\n",
    "        search = RandomizedSearchCV(\n",
    "            base_model, param_dist, n_iter=30, cv=cv_inner,\n",
    "            scoring='r2', random_state=RANDOM_STATE, n_jobs=-1, refit=True\n",
    "        )\n",
    "        search.fit(X, y)\n",
    "        score = search.best_score_\n",
    "        print(f'  {name:20s} best CV R²: {score:.4f}')\n",
    "\n",
    "        tuning_results.append({\n",
    "            'Target': target, 'Model': name,\n",
    "            'Best_R2_CV': round(score, 4),\n",
    "            'Best_Params': str(search.best_params_)\n",
    "        })\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_name = name\n",
    "            best_est = search.best_estimator_\n",
    "\n",
    "    best_models[target] = (best_name, best_est)\n",
    "    print(f'  >>> Best for {target}: {best_name} (R²={best_score:.4f})')\n",
    "\n",
    "tuning_df = pd.DataFrame(tuning_results)\n",
    "print('\\n' + '=' * 80)\n",
    "print('Tuning Summary:')\n",
    "tuning_df[['Target', 'Model', 'Best_R2_CV']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## Step 13: Final Comparison — All Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = pd.concat([\n",
    "    baseline_df.assign(Experiment='1. Baseline (4 feat)'),\n",
    "    curated_df.assign(Experiment='2. Curated RF (12 feat)'),\n",
    "    log_df.assign(Experiment='3. Curated log1p RF'),\n",
    "], ignore_index=True)\n",
    "\n",
    "for target in TARGETS:\n",
    "    name, est = best_models[target]\n",
    "    trow = tuning_df[(tuning_df['Target'] == target) & (tuning_df['Model'] == name)].iloc[0]\n",
    "    all_results = pd.concat([all_results, pd.DataFrame([{\n",
    "        'Model': f'Tuned {name}',\n",
    "        'Target': target,\n",
    "        'R2_mean': trow['Best_R2_CV'],\n",
    "        'R2_std': np.nan,\n",
    "        'RMSE_mean': np.nan,\n",
    "        'RMSE_std': np.nan,\n",
    "        'Experiment': '4. Tuned best model',\n",
    "    }])], ignore_index=True)\n",
    "\n",
    "print('\\nFull Comparison:')\n",
    "pivot = all_results.pivot_table(index='Target', columns='Experiment', values='R2_mean')\n",
    "print(pivot.to_string())\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "for i, target in enumerate(TARGETS):\n",
    "    subset = all_results[all_results['Target'] == target].sort_values('R2_mean')\n",
    "    axes[i].barh(subset['Experiment'], subset['R2_mean'])\n",
    "    axes[i].set_xlabel('R² (CV mean)')\n",
    "    axes[i].set_title(target)\n",
    "    axes[i].set_xlim(0, max(0.8, subset['R2_mean'].max() + 0.05))\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## Step 14: Permutation Feature Importance (Best Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 7))\n",
    "\n",
    "for i, target in enumerate(TARGETS):\n",
    "    name, model = best_models[target]\n",
    "    X = train_df[CURATED_FEATURES].values\n",
    "    y = train_df[target].values\n",
    "\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=RANDOM_STATE)\n",
    "    model.fit(X_tr, y_tr)\n",
    "\n",
    "    perm_imp = permutation_importance(\n",
    "        model, X_te, y_te, n_repeats=10, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "\n",
    "    sorted_idx = perm_imp.importances_mean.argsort()\n",
    "    axes[i].boxplot(\n",
    "        perm_imp.importances[sorted_idx].T,\n",
    "        vert=False,\n",
    "        labels=np.array(CURATED_FEATURES)[sorted_idx])\n",
    "    axes[i].set_title(f'{target}\\n({name})')\n",
    "    axes[i].set_xlabel('Permutation Importance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance_enhanced.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## Step 15: Generate Submission Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = build_frame(\n",
    "    submission_template[['Latitude', 'Longitude', 'Sample Date']],\n",
    "    landsat_val, tc_val\n",
    ")\n",
    "val_df = add_temporal_features(val_df)\n",
    "\n",
    "val_df['landsat_missing'] = val_df['nir'].isnull().astype(float)\n",
    "val_df = val_df.fillna(train_df.median(numeric_only=True))\n",
    "\n",
    "val_df['Cluster'] = kmeans.predict(val_df[['Latitude', 'Longitude']]).astype(float)\n",
    "\n",
    "print(f'Validation shape: {val_df.shape}')\n",
    "print(f'Missing after impute: {val_df[CURATED_FEATURES].isnull().sum().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full = train_df[CURATED_FEATURES].values\n",
    "X_val = val_df[CURATED_FEATURES].values\n",
    "\n",
    "predictions = {}\n",
    "for target in TARGETS:\n",
    "    name, model = best_models[target]\n",
    "    y_full = train_df[target].values\n",
    "    model.fit(X_train_full, y_full)\n",
    "    preds = model.predict(X_val)\n",
    "    preds = np.maximum(preds, 0)\n",
    "    predictions[target] = preds\n",
    "    print(f'{target:35s} — {name:20s} — '\n",
    "          f'mean={preds.mean():.2f}, std={preds.std():.2f}, '\n",
    "          f'min={preds.min():.2f}, max={preds.max():.2f}')\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'Latitude': submission_template['Latitude'].values,\n",
    "    'Longitude': submission_template['Longitude'].values,\n",
    "    'Sample Date': submission_template['Sample Date'].values,\n",
    "    'Total Alkalinity': predictions['Total Alkalinity'],\n",
    "    'Electrical Conductance': predictions['Electrical Conductance'],\n",
    "    'Dissolved Reactive Phosphorus': predictions['Dissolved Reactive Phosphorus'],\n",
    "})\n",
    "\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "print(f'\\nSubmission saved: submission.csv ({submission_df.shape})')\n",
    "submission_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "## Step 16: Submission Distribution Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "for i, target in enumerate(TARGETS):\n",
    "    axes[i].hist(train_df[target].values, bins=50, alpha=0.5, label='Training', density=True)\n",
    "    axes[i].hist(predictions[target], bins=30, alpha=0.7, label='Predicted', density=True)\n",
    "    axes[i].set_title(target)\n",
    "    axes[i].legend()\n",
    "    axes[i].set_xlabel('Value')\n",
    "    axes[i].set_ylabel('Density')\n",
    "plt.tight_layout()\n",
    "plt.savefig('submission_distributions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('Done. All phases complete.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
